{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61a0720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a120102",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/conda/envs/dacslab_djk_llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "from pyfunctions.cdt_basic import *\n",
    "from pyfunctions.cdt_source_to_target import *\n",
    "from pyfunctions.ioi_dataset import IOIDataset\n",
    "from pyfunctions.wrappers import Node, AblationSet\n",
    "from pyfunctions.faithfulness_ablations import logits_to_ave_logit_diff_2, add_mean_ablation_hook\n",
    "\n",
    "\n",
    "Result = collections.namedtuple('Result', ('ablation_set', 'score'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7651660-7f59-4b59-a574-afecc52dc306",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a520f760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "# Model code adapted from Callum McDougall's notebook for ARENA on reproducing the IOI paper using TransformerLens.\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\",\n",
    "                                          center_unembed=True,\n",
    "                                          center_writing_weights=True,\n",
    "                                          fold_ln=False,\n",
    "                                          refactor_factored_attn_matrices=True)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffd4d3-5e8f-4587-bb2a-f06b61918c09",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Generate mean activations / Example usage of the IOI dataset\n",
    "\n",
    "This is not as simple as it sounds; for the IOI paper, for each individual input following a template, they ablate using the mean activations of the \"ABC\" dataset, generated over sentences following the same template.\n",
    "\n",
    "For those who are familiar with usage of the IOI dataset code, our code is not designed to take advantage of the IOI dataset's sequence position labels (it fundamentally can't be because our method is automated and therefore can't incorporate knowledge of the sequence position labels, i.e, we can find that unlabeled positions are relevant), so circuit analysis needs to be done on a per-template basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab88048",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 16, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a dataset all consisting of one template, randomly chosen.\n",
    "# nb_templates = 2 due to some logic internal to IOIDataset:\n",
    "# essentially, the nouns can be an ABBA or ABAB order and that counts as separate templates.\n",
    "ioi_dataset = IOIDataset(prompt_type=\"mixed\", N=50, tokenizer=model.tokenizer, prepend_bos=False, nb_templates=2)\n",
    "\n",
    "# This is the P_ABC that is mentioned in the IOI paper, which we use for mean ablation.\n",
    "# Importantly, passing in prompt_type=\"ABC\" or similar is NOT the same thing as this.\n",
    "abc_dataset = (\n",
    "    ioi_dataset.gen_flipped_prompts((\"IO\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S1\", \"RAND\"))\n",
    ")\n",
    "\n",
    "logits, cache = model.run_with_cache(abc_dataset.toks) # run on entire dataset along batch dimension\n",
    "\n",
    "# A technical detail: We patch at what TLens calls the \"z\" activation in the attention, which if you think about it is the only natural way to patch attention outputs on a per-head basis with the standard attention implementation that doesn't have a separate dimension for attention heads.\n",
    "attention_outputs = [cache['blocks.' + str(i) + '.attn.hook_z'] for i in range(12)]\n",
    "attention_outputs = torch.stack(attention_outputs, dim=1) # now batch, layer, seq, n_heads, dim_attn\n",
    "mean_acts = torch.mean(attention_outputs, dim=0)\n",
    "\n",
    "# different implementations of attention have a separate dimension for the attention heads, and we need to make sure the shapes are as expected.\n",
    "old_shape = mean_acts.shape\n",
    "last_dim = old_shape[-2] * old_shape[-1]\n",
    "new_shape = old_shape[:-2] + (last_dim,)\n",
    "mean_acts = mean_acts.view(new_shape)\n",
    "mean_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c32de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = ioi_dataset.sentences[0]\n",
    "encoding = model.tokenizer.encode_plus(text, \n",
    "                                 add_special_tokens=True, \n",
    "                                 max_length=512,\n",
    "                                 truncation=True, \n",
    "                                 padding = \"longest\", \n",
    "                                 return_attention_mask=True, \n",
    "                                 return_tensors=\"pt\").to(device)\n",
    "encoding_idxs, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "input_shape = encoding_idxs.size()\n",
    "extended_attention_mask = get_extended_attention_mask(attention_mask, \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea7c8f-a673-4b44-b9a9-d8e7615b3ec5",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Analysis\n",
    "\n",
    "These cells define the two basic operations of our method: decomposing the contribution directly to the logits, and decomposing the contribution to given target nodes.\n",
    "If you want to perform a specific analysis that requires some degree of human intervention or heuristic pruning, these cells are the place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2e4613-362f-480e-95fb-4f353ac3a845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running decomposition in batches...: 100%|██████████| 36/36 [00:07<00:00,  4.79it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ranges = [\n",
    "        [layer for layer in range(12)],\n",
    "        [sequence_position for sequence_position in range(input_shape[1])],\n",
    "        [attention_head_idx for attention_head_idx in range(12)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "ablation_sets = [(n,) for n in source_nodes]\n",
    "\n",
    "target_nodes = []\n",
    "\n",
    "# cache activations for faster batch run\n",
    "out_decomp, _, _, pre_layer_activations = prop_GPT(encoding_idxs[0:1, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True)\n",
    "\n",
    "prop_fn = lambda ablation_list: prop_GPT(encoding_idxs[0:1, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "out_decomps, target_decomps = batch_run(prop_fn, ablation_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b6f2e3-76e4-4f71-8f57-a87e6cbea335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_logits_decomposition_scores(out_decomps):\n",
    "    logits = (out_decomps[0].rel + out_decomps[0].irrel) # 1, seq_len, 50257=d_vocab\n",
    "    io_logit = logits[0, -2, ioi_dataset.io_tokenIDs[0]]\n",
    "    s_logit = logits[0, -2, ioi_dataset.s_tokenIDs[0]]\n",
    "    full_score = np.abs(io_logit - s_logit)\n",
    "    assert(full_score > 0) # GPT2 succeeds at this 99%+ of the time but not always. If you are doing analysis over a batch it mostly won't make a difference.\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for decomp in out_decomps:\n",
    "        rel_io_logit = decomp.rel[0, -2, ioi_dataset.io_tokenIDs[0]]\n",
    "        rel_s_logit = decomp.rel[0, -2, ioi_dataset.s_tokenIDs[0]]\n",
    "        score = rel_io_logit - rel_s_logit\n",
    "        results.append(Result(decomp.ablation_set, score))\n",
    "    results.sort(key=operator.attrgetter('score'), reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca897ab1-f348-4a6e-a43a-96f0fc30c0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(ablation_set=(Node(layer_idx=9, sequence_idx=14, attn_head_idx=9),), score=1.3629175)\n",
      "Result(ablation_set=(Node(layer_idx=10, sequence_idx=14, attn_head_idx=0),), score=0.64960474)\n",
      "Result(ablation_set=(Node(layer_idx=10, sequence_idx=14, attn_head_idx=6),), score=0.57740194)\n",
      "Result(ablation_set=(Node(layer_idx=9, sequence_idx=14, attn_head_idx=6),), score=0.46229994)\n",
      "Result(ablation_set=(Node(layer_idx=9, sequence_idx=14, attn_head_idx=8),), score=0.37956655)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=4, attn_head_idx=1),), score=0.32436788)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=3, attn_head_idx=7),), score=0.30916834)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=3, attn_head_idx=4),), score=0.30499387)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=4, attn_head_idx=5),), score=0.3039614)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=3, attn_head_idx=6),), score=0.30057526)\n"
     ]
    }
   ],
   "source": [
    "results = compute_logits_decomposition_scores(out_decomps)\n",
    "\n",
    "for result in results[:10]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb15008-7d61-4bfd-b0e4-7126bd08951a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers_per_iter = []\n",
    "results_per_iter = [results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32b2b05c-eee7-4a95-aef5-126b082ec550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node(layer_idx=9, sequence_idx=14, attn_head_idx=9), Node(layer_idx=10, sequence_idx=14, attn_head_idx=0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running decomposition in batches...: 100%|██████████| 36/36 [00:07<00:00,  4.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now, find maximally relevant source nodes to target nodes\n",
    "\n",
    "outliers = results[:2] # say we take top 2 heads in each iter\n",
    "outliers_per_iter.append(outliers)\n",
    "target_nodes = [r.ablation_set[0] for r in outliers] # here we assume that we only ever tried to ablate one node at once, but our code support multiple node ablation at once too\n",
    "print(target_nodes)\n",
    "ranges = [\n",
    "        [layer for layer in range(12)],\n",
    "        [sequence_position for sequence_position in range(16)],\n",
    "        # [ioi_dataset.word_idx['IO'][0]],\n",
    "        [attention_head_idx for attention_head_idx in range(12)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "ablation_sets = [(n,) for n in source_nodes]\n",
    "\n",
    "_, _, _, pre_layer_activations = prop_GPT(encoding_idxs[0:1, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True)\n",
    "\n",
    "prop_fn = lambda ablation_list: prop_GPT(encoding_idxs[0:1, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "out_decomps, target_decomps = batch_run(prop_fn, ablation_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ebab7b5-5f80-40b6-98f1-bccf4a16274a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_target_decomposition_scores(target_decomps, method=\"l1\", mean_acts=None, attn_cache=None):\n",
    "    results = []\n",
    "    relevances = np.zeros((12, 16, 12))\n",
    "    for layer in range(12):\n",
    "        for sequence_position in range(16):\n",
    "            for attention_head_idx in range(12):\n",
    "                idx = layer * 16 * 12 + sequence_position * 12 + attention_head_idx\n",
    "                target_decomp = target_decomps[idx]\n",
    "                if target_decomp.ablation_set[0] in target_nodes:\n",
    "                    continue\n",
    "                score = 0\n",
    "                for i in range(len(target_decomp.target_nodes)):\n",
    "                    if method == 'l1':\n",
    "                        rels_magnitude = torch.mean(abs(target_decomp.rels[i])) # np.mean if you are on cpu\n",
    "                        irrels_magnitude = torch.mean(abs(target_decomp.irrels[i])) # np.mean if you are on cpu\n",
    "                        target_node_score = rels_magnitude / (rels_magnitude + irrels_magnitude)\n",
    "                        score += target_node_score\n",
    "                    if method == 'dot':\n",
    "                        target_node = target_decomp.target_nodes[i]\n",
    "                        # this method is only implemented for a single datapoint\n",
    "                        if mean_acts is None or attn_cache is None:\n",
    "                            print(\"Invalid target decomposition score calculation\") # and then this is going to crash anyway\n",
    "                        target_mean_act = mean_acts[target_node.layer_idx, target_node.sequence_idx, target_node.attn_head_idx]\n",
    "                        target_rel = attn_cache['blocks.' + str(target_node.layer_idx) + '.attn.hook_z'][0][target_node.sequence_idx][target_node.attn_head_idx] - target_mean_act \n",
    "                        rel = target_decomp.rels[i][0]\n",
    "                        #print(target_rel.shape, rel.shape)\n",
    "                        score += torch.dot(rel, target_rel)\n",
    "                relevances[layer, sequence_position, attention_head_idx] = score\n",
    "\n",
    "\n",
    "    sums_per_layer = np.sum(relevances, axis=(1, 2))\n",
    "    sums_per_layer[sums_per_layer == 0] = -1e-8\n",
    "    normalized_relevances = relevances / np.expand_dims(sums_per_layer, (1, 2))\n",
    "\n",
    "    num_layers = 12\n",
    "    seq_len = 16\n",
    "    num_attention_heads = 12\n",
    "    for layer_idx in range(num_layers):\n",
    "        for seq_pos in range(seq_len):\n",
    "            for head_idx in range(num_attention_heads):\n",
    "                target_decomp = target_decomps[layer_idx * seq_len * num_attention_heads + seq_pos * num_attention_heads + head_idx]\n",
    "                results.append(Result(target_decomp.ablation_set, normalized_relevances[layer_idx, seq_pos, head_idx]))\n",
    "\n",
    "    results.sort(key=operator.attrgetter('score'), reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd696fa-86b4-4ba3-862f-9be581d67a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Result(ablation_set=(Node(layer_idx=9, sequence_idx=14, attn_head_idx=9),), score=1.3629175),\n",
       "  Result(ablation_set=(Node(layer_idx=10, sequence_idx=14, attn_head_idx=0),), score=0.64960474)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers_per_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "587a4627-5729-4810-aaad-5445007a9869",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(ablation_set=(Node(layer_idx=9, sequence_idx=14, attn_head_idx=9),), score=1.3629175)\n",
      "Result(ablation_set=(Node(layer_idx=10, sequence_idx=14, attn_head_idx=0),), score=0.64960474)\n",
      "Result(ablation_set=(Node(layer_idx=10, sequence_idx=14, attn_head_idx=6),), score=0.57740194)\n",
      "Result(ablation_set=(Node(layer_idx=9, sequence_idx=14, attn_head_idx=6),), score=0.46229994)\n",
      "Result(ablation_set=(Node(layer_idx=9, sequence_idx=14, attn_head_idx=8),), score=0.37956655)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=4, attn_head_idx=1),), score=0.32436788)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=3, attn_head_idx=7),), score=0.30916834)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=3, attn_head_idx=4),), score=0.30499387)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=4, attn_head_idx=5),), score=0.3039614)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=3, attn_head_idx=6),), score=0.30057526)\n"
     ]
    }
   ],
   "source": [
    "for result in results[:10]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15844abd-6da0-491c-b8f3-b58444a0f11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "for it in outliers_per_iter:\n",
    "    for result in it:\n",
    "        if result.ablation_set[0] not in all_nodes:\n",
    "            all_nodes.append(result.ablation_set[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e32e8-925d-4285-b6db-d54e6854a531",
   "metadata": {},
   "source": [
    "## Automatic search\n",
    "\n",
    "This is just a bunch of the above cells put into a neat cell that automatically finds some sort of circuit without any manual intervention.\n",
    "As explained above, the code is not designed to take advantage of the IOI dataset's sequence position labels, so circuit analysis needs to be done on a per-template basis; here a template is hardcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05babeb3-e7fb-414b-8ad4-0701b77cfd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence length: 16 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running decomposition in batches...: 100%|██████████| 36/36 [00:04<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node(layer_idx=10, sequence_idx=14, attn_head_idx=0), Node(layer_idx=9, sequence_idx=14, attn_head_idx=9)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running decomposition in batches...: 100%|██████████| 36/36 [00:05<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node(layer_idx=9, sequence_idx=11, attn_head_idx=6), Node(layer_idx=9, sequence_idx=1, attn_head_idx=8)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running decomposition in batches...: 100%|██████████| 36/36 [00:08<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node(layer_idx=1, sequence_idx=1, attn_head_idx=10), Node(layer_idx=1, sequence_idx=1, attn_head_idx=11)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running decomposition in batches...: 100%|██████████| 36/36 [00:03<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node(layer_idx=0, sequence_idx=1, attn_head_idx=1), Node(layer_idx=0, sequence_idx=1, attn_head_idx=4)]\n"
     ]
    }
   ],
   "source": [
    "from pyfunctions.ioi_dataset import ABC_TEMPLATES, BAC_TEMPLATES, BABA_TEMPLATES, BABA_LONG_TEMPLATES, BABA_LATE_IOS, BABA_EARLY_IOS, ABBA_TEMPLATES, ABBA_LATE_IOS, ABBA_EARLY_IOS\n",
    "\n",
    "model.reset_hooks(including_permanent=True)\n",
    "\n",
    "NUM_SAMPLES = 1\n",
    "NUM_OUTLIERS_TO_KEEP_PER_ITER = 2\n",
    "template = ABBA_EARLY_IOS[0]\n",
    "ioi_dataset = IOIDataset(N=50, tokenizer=model.tokenizer, prepend_bos=False, prompt_type=[template])\n",
    "\n",
    "# This is the P_ABC that is mentioned in the IOI paper, which we use for mean ablation.\n",
    "# Importantly, passing in prompt_type=\"ABC\" or similar is NOT the same thing as this.\n",
    "abc_dataset = (\n",
    "    ioi_dataset.gen_flipped_prompts((\"IO\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S1\", \"RAND\"))\n",
    ")\n",
    "ioi_logits, ioi_cache = model.run_with_cache(ioi_dataset.toks) # run on entire dataset along batch dimension\n",
    "logits, cache = model.run_with_cache(abc_dataset.toks) # run on entire dataset along batch dimension\n",
    "\n",
    "attention_outputs = [cache['blocks.' + str(i) + '.attn.hook_z'] for i in range(12)]\n",
    "attention_outputs = torch.stack(attention_outputs, dim=1) # now batch, layer, seq, n_heads, dim_attn\n",
    "mean_acts = torch.mean(attention_outputs, dim=0)\n",
    "old_shape = mean_acts.shape\n",
    "last_dim = old_shape[-2] * old_shape[-1]\n",
    "new_shape = old_shape[:-2] + (last_dim,)\n",
    "mean_acts = mean_acts.view(new_shape)\n",
    "\n",
    "text = ioi_dataset.sentences[0]\n",
    "encoding = model.tokenizer.encode_plus(text, \n",
    "                                 add_special_tokens=True, \n",
    "                                 max_length=512,\n",
    "                                 truncation=True, \n",
    "                                 padding = \"longest\", \n",
    "                                 return_attention_mask=True, \n",
    "                                 return_tensors=\"pt\").to(device)\n",
    "input_shape = encoding.input_ids.size()\n",
    "extended_attention_mask = get_extended_attention_mask(encoding.attention_mask, \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)\n",
    "seq_len = ioi_dataset.toks.shape[1]\n",
    "print('sequence length: %d ' % seq_len)\n",
    "# Calculate relevance to logits\n",
    "ranges = [\n",
    "        [layer for layer in range(12)],\n",
    "        [sequence_position for sequence_position in range(seq_len)],\n",
    "        [attention_head_idx for attention_head_idx in range(12)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "ablation_sets = [(n,) for n in source_nodes]\n",
    "target_nodes = []\n",
    "\n",
    "# cache activations for faster batch run\n",
    "out_decomp, _, _, pre_layer_activations = prop_GPT(ioi_dataset.toks[0:NUM_SAMPLES, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True)\n",
    "prop_fn = lambda ablation_list: prop_GPT(ioi_dataset.toks[0:NUM_SAMPLES, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "out_decomps, _ = batch_run(prop_fn, ablation_sets, num_at_time=(64 // NUM_SAMPLES))\n",
    "results = compute_logits_decomposition_scores(out_decomps)\n",
    "\n",
    "\n",
    "# This loop implements a simple heuristic of keeping a hardcoded top N outliers from each iteration.\n",
    "# It terminates when all the nodes are in the first layer, so it has the shortcoming of continually trying to find nodes even when they are not necessarily important.\n",
    "# Various heuristic techniques, such as filtering nodes by how their relevance scores compare to others in the same layer, or same iteration, can be applied.\n",
    "# It is also possible to implement early stopping or other heuristic techniques based on the circuit's performance.\n",
    "\n",
    "\n",
    "outliers_per_iter = []\n",
    "while True:\n",
    "    outliers = results[:NUM_OUTLIERS_TO_KEEP_PER_ITER]\n",
    "    outliers_per_iter.append(outliers)\n",
    "    target_nodes = [r.ablation_set[0] for r in outliers]\n",
    "    print(target_nodes)\n",
    "    should_break = True\n",
    "    for node in target_nodes:\n",
    "        if node.layer_idx != 0:\n",
    "            should_break = False\n",
    "    if should_break:\n",
    "        break\n",
    "\n",
    "    # In this loop, we implement search over all sequence positions.\n",
    "    # This result is less stable than the one augmented by some amount of manual analysis.\n",
    "    ranges = [\n",
    "            [layer for layer in range(12)],\n",
    "            [sequence_position for sequence_position in range(seq_len)],\n",
    "            [attention_head_idx for attention_head_idx in range(12)]\n",
    "        ]\n",
    "    source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "    ablation_sets = [(n,) for n in source_nodes]\n",
    "    prop_fn = lambda ablation_list: prop_GPT(ioi_dataset.toks[0:NUM_SAMPLES, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "    _, target_decomps = batch_run(prop_fn, ablation_sets, num_at_time=(64 // NUM_SAMPLES))\n",
    "    \n",
    "    results = calculate_target_decomposition_scores(target_decomps, method=\"dot\", mean_acts=mean_acts.view(old_shape), attn_cache=ioi_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15bb0c2f-bef4-4bc3-8f5a-f2177a17e538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node(layer_idx=10, sequence_idx=14, attn_head_idx=0)\n",
      "Node(layer_idx=9, sequence_idx=14, attn_head_idx=9)\n",
      "Node(layer_idx=9, sequence_idx=11, attn_head_idx=6)\n",
      "Node(layer_idx=9, sequence_idx=1, attn_head_idx=8)\n",
      "Node(layer_idx=1, sequence_idx=1, attn_head_idx=10)\n",
      "Node(layer_idx=1, sequence_idx=1, attn_head_idx=11)\n",
      "Node(layer_idx=0, sequence_idx=1, attn_head_idx=1)\n",
      "Node(layer_idx=0, sequence_idx=1, attn_head_idx=4)\n"
     ]
    }
   ],
   "source": [
    "all_nodes = []\n",
    "for it in outliers_per_iter:\n",
    "    for result in it:\n",
    "        if result.ablation_set[0] not in all_nodes:\n",
    "            all_nodes.append(result.ablation_set[0])\n",
    "for node in all_nodes:\n",
    "    print((node))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac83979",
   "metadata": {},
   "source": [
    "# Circuit evaluation\n",
    "\n",
    "Most of the actual evaluation code is implemented in the IOI repo; we just make calls to convenient functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b526010-d4e4-41b3-9ff2-fe33d832e148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "circuit = [Node(layer_idx=8, sequence_idx=14, attn_head_idx=6),\n",
    "           Node(layer_idx=8, sequence_idx=11, attn_head_idx=6),\n",
    "           Node(layer_idx=9, sequence_idx=14, attn_head_idx=9),\n",
    "           Node(layer_idx=9, sequence_idx=14, attn_head_idx=6),\n",
    "           Node(layer_idx=5, sequence_idx=10, attn_head_idx=5),\n",
    "           Node(layer_idx=7, sequence_idx=11, attn_head_idx=9),\n",
    "           Node(layer_idx=6, sequence_idx=10, attn_head_idx=9),\n",
    "           Node(layer_idx=6, sequence_idx=11, attn_head_idx=0),\n",
    "           Node(layer_idx=5, sequence_idx=10, attn_head_idx=9),\n",
    "           Node(layer_idx=3, sequence_idx=10, attn_head_idx=0),\n",
    "           Node(layer_idx=4, sequence_idx=5, attn_head_idx=11),\n",
    "           Node(layer_idx=3, sequence_idx=5, attn_head_idx=7),\n",
    "           Node(layer_idx=3, sequence_idx=3, attn_head_idx=6),\n",
    "           Node(layer_idx=2, sequence_idx=3, attn_head_idx=2),\n",
    "           Node(layer_idx=2, sequence_idx=3, attn_head_idx=9),\n",
    "           Node(layer_idx=1, sequence_idx=3, attn_head_idx=7),\n",
    "           Node(layer_idx=1, sequence_idx=3, attn_head_idx=10),\n",
    "           Node(layer_idx=0, sequence_idx=2, attn_head_idx=1),\n",
    "           Node(layer_idx=0, sequence_idx=2, attn_head_idx=4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33374a25-c2a8-493e-8480-ecc2274e6ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_circuit = random.sample(source_nodes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9db57878-aaf0-4ef6-83ad-021902bf9e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4622, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# This template definitely has to match the template used in the search above, otherwise, the sequence positions will not be validly interpretable.\n",
    "test_ioi_dataset = IOIDataset(prompt_type=[template], N=10, tokenizer=model.tokenizer, prepend_bos=False)\n",
    "test_abc_dataset = (\n",
    "    test_ioi_dataset.gen_flipped_prompts((\"IO\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S1\", \"RAND\"))\n",
    ")\n",
    "\n",
    "circuit = all_nodes\n",
    "\n",
    "model.reset_hooks(including_permanent=True)\n",
    "model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset, circuit=circuit) #, circuit=random_circuit)\n",
    "# model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset)\n",
    "logits, cache = model.run_with_cache(test_ioi_dataset.toks) # run on entire dataset along batch dimension\n",
    "ave_logit_diff = logits_to_ave_logit_diff_2(logits, test_ioi_dataset)\n",
    "print(ave_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7de9d9f5-2955-46b7-92bc-9f26308ce450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNode(layer_idx=9, sequence_idx=14, attn_head_idx=9)\\nNode(layer_idx=9, sequence_idx=14, attn_head_idx=6)\\nNode(layer_idx=10, sequence_idx=14, attn_head_idx=0)\\nNode(layer_idx=8, sequence_idx=1, attn_head_idx=11)\\nNode(layer_idx=8, sequence_idx=1, attn_head_idx=10)\\nNode(layer_idx=8, sequence_idx=1, attn_head_idx=2)\\nNode(layer_idx=7, sequence_idx=1, attn_head_idx=1)\\nNode(layer_idx=7, sequence_idx=1, attn_head_idx=4)\\nNode(layer_idx=6, sequence_idx=1, attn_head_idx=4)\\nNode(layer_idx=6, sequence_idx=1, attn_head_idx=0)\\nNode(layer_idx=5, sequence_idx=1, attn_head_idx=10)\\nNode(layer_idx=5, sequence_idx=1, attn_head_idx=2)\\nNode(layer_idx=5, sequence_idx=1, attn_head_idx=3)\\nNode(layer_idx=5, sequence_idx=1, attn_head_idx=6)\\nNode(layer_idx=5, sequence_idx=1, attn_head_idx=9)\\nNode(layer_idx=4, sequence_idx=1, attn_head_idx=3)\\nNode(layer_idx=4, sequence_idx=1, attn_head_idx=10)\\nNode(layer_idx=4, sequence_idx=1, attn_head_idx=9)\\nNode(layer_idx=1, sequence_idx=1, attn_head_idx=3)\\nNode(layer_idx=1, sequence_idx=1, attn_head_idx=10)\\nNode(layer_idx=1, sequence_idx=1, attn_head_idx=4)\\nNode(layer_idx=0, sequence_idx=1, attn_head_idx=3)\\nNode(layer_idx=0, sequence_idx=1, attn_head_idx=4)\\nNode(layer_idx=0, sequence_idx=1, attn_head_idx=5)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: for the following circuit:\n",
    "'''\n",
    "Node(layer_idx=9, sequence_idx=14, attn_head_idx=9)\n",
    "Node(layer_idx=9, sequence_idx=14, attn_head_idx=6)\n",
    "Node(layer_idx=10, sequence_idx=14, attn_head_idx=0)\n",
    "Node(layer_idx=8, sequence_idx=1, attn_head_idx=11)\n",
    "Node(layer_idx=8, sequence_idx=1, attn_head_idx=10)\n",
    "Node(layer_idx=8, sequence_idx=1, attn_head_idx=2)\n",
    "Node(layer_idx=7, sequence_idx=1, attn_head_idx=1)\n",
    "Node(layer_idx=7, sequence_idx=1, attn_head_idx=4)\n",
    "Node(layer_idx=6, sequence_idx=1, attn_head_idx=4)\n",
    "Node(layer_idx=6, sequence_idx=1, attn_head_idx=0)\n",
    "Node(layer_idx=5, sequence_idx=1, attn_head_idx=10)\n",
    "Node(layer_idx=5, sequence_idx=1, attn_head_idx=2)\n",
    "Node(layer_idx=5, sequence_idx=1, attn_head_idx=3)\n",
    "Node(layer_idx=5, sequence_idx=1, attn_head_idx=6)\n",
    "Node(layer_idx=5, sequence_idx=1, attn_head_idx=9)\n",
    "Node(layer_idx=4, sequence_idx=1, attn_head_idx=3)\n",
    "Node(layer_idx=4, sequence_idx=1, attn_head_idx=10)\n",
    "Node(layer_idx=4, sequence_idx=1, attn_head_idx=9)\n",
    "Node(layer_idx=1, sequence_idx=1, attn_head_idx=3)\n",
    "Node(layer_idx=1, sequence_idx=1, attn_head_idx=10)\n",
    "Node(layer_idx=1, sequence_idx=1, attn_head_idx=4)\n",
    "Node(layer_idx=0, sequence_idx=1, attn_head_idx=3)\n",
    "Node(layer_idx=0, sequence_idx=1, attn_head_idx=4)\n",
    "Node(layer_idx=0, sequence_idx=1, attn_head_idx=5)\n",
    "'''\n",
    "# removing just one node, (8, 1, 11), raises the score from -2.1718 to -0.4423.\n",
    "# this node is not identified by the IOI paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04fa93",
   "metadata": {},
   "source": [
    "# Pruning heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b5c5eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tentatively improved score to -0.462197   by removing node  Node(layer_idx=9, sequence_idx=11, attn_head_idx=6)\n",
      "tentatively improved score to -0.437711   by removing node  Node(layer_idx=0, sequence_idx=1, attn_head_idx=4)\n",
      "removing  Node(layer_idx=0, sequence_idx=1, attn_head_idx=4)  to achieve score of -0.437711\n",
      "tentatively improved score to -0.437698   by removing node  Node(layer_idx=9, sequence_idx=11, attn_head_idx=6)\n",
      "removing  Node(layer_idx=9, sequence_idx=11, attn_head_idx=6)  to achieve score of -0.437698\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Prune nodes by greedy search to form a better circuit\n",
    "\n",
    "NAME_MOVER_HEADS = [Node(9, 14, 9), Node(10, 14, 0), Node(9, 14, 6)]\n",
    "old_circuit = circuit.copy()\n",
    "best_score = -1.4686 # \n",
    "while True:\n",
    "    node_to_remove = None\n",
    "    for idx, node in enumerate(circuit):\n",
    "        if node in NAME_MOVER_HEADS:\n",
    "            continue\n",
    "        new_circuit = circuit.copy()\n",
    "        new_circuit.remove(node)\n",
    "        # print(new_circuit)\n",
    "        model.reset_hooks(including_permanent=True)\n",
    "        model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset, circuit=new_circuit)\n",
    "        logits, cache = model.run_with_cache(test_ioi_dataset.toks) # run on entire dataset along batch dimension\n",
    "        ave_logit_diff = logits_to_ave_logit_diff_2(logits, test_ioi_dataset).cpu().numpy().item()\n",
    "        if ave_logit_diff > best_score:\n",
    "            best_score = ave_logit_diff\n",
    "            node_to_remove = node\n",
    "            print('tentatively improved score to %f ' % best_score, ' by removing node ', node_to_remove)\n",
    "    if node_to_remove is None: \n",
    "        # then we can't improve any further so the algorithm terminates\n",
    "        break\n",
    "    print(\"removing \", node_to_remove, \" to achieve score of %f\" % best_score)\n",
    "    circuit.remove(node_to_remove)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e9717bb-1f4e-4823-bcfc-ea685daf8af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4377, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.reset_hooks(including_permanent=True)\n",
    "model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset, circuit=circuit)\n",
    "# model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset, circuit=nodes)\n",
    "logits, cache = model.run_with_cache(test_ioi_dataset.toks) # run on entire dataset along batch dimension\n",
    "ave_logit_diff = logits_to_ave_logit_diff_2(logits, test_ioi_dataset)\n",
    "print(ave_logit_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacslab_djk_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
