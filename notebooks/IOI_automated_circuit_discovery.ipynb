{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61a0720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a120102",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extractListFromDic, readJson, combine_token_attn, compute_word_intervals, compare_same\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpathology\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extract_synoptic, fixLabelProstateGleason, fixProstateLabels, fixLabel, exclude_labels\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcdt_basic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcdt_source_to_target\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mioi_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IOIDataset\n",
      "File \u001b[0;32m~/CD_Circuit/pyfunctions/cdt_basic.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfancy_einsum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m einsum\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModuleUtilsMixin\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# utility\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformer_lens/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hook_points\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evals\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformer_lens/hook_points.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Slice, SliceInput\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLensHandle\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Dataclass that holds information about a PyTorch hook.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformer_lens/utils.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaxtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Float, Int\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrich\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mprint\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m rprint\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFactoredMatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FactoredMatrix\n\u001b[1;32m     31\u001b[0m CACHE_DIR \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTRANSFORMERS_CACHE\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1591\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1605\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1606\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1607\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1608\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/linux/miniforge-3.12/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     albert,\n\u001b[1;32m     17\u001b[0m     align,\n\u001b[1;32m     18\u001b[0m     altclip,\n\u001b[1;32m     19\u001b[0m     audio_spectrogram_transformer,\n\u001b[1;32m     20\u001b[0m     auto,\n\u001b[1;32m     21\u001b[0m     autoformer,\n\u001b[1;32m     22\u001b[0m     bark,\n\u001b[1;32m     23\u001b[0m     bart,\n\u001b[1;32m     24\u001b[0m     barthez,\n\u001b[1;32m     25\u001b[0m     bartpho,\n\u001b[1;32m     26\u001b[0m     beit,\n\u001b[1;32m     27\u001b[0m     bert,\n\u001b[1;32m     28\u001b[0m     bert_generation,\n\u001b[1;32m     29\u001b[0m     bert_japanese,\n\u001b[1;32m     30\u001b[0m     bertweet,\n\u001b[1;32m     31\u001b[0m     big_bird,\n\u001b[1;32m     32\u001b[0m     bigbird_pegasus,\n\u001b[1;32m     33\u001b[0m     biogpt,\n\u001b[1;32m     34\u001b[0m     bit,\n\u001b[1;32m     35\u001b[0m     blenderbot,\n\u001b[1;32m     36\u001b[0m     blenderbot_small,\n\u001b[1;32m     37\u001b[0m     blip,\n\u001b[1;32m     38\u001b[0m     blip_2,\n\u001b[1;32m     39\u001b[0m     bloom,\n\u001b[1;32m     40\u001b[0m     bridgetower,\n\u001b[1;32m     41\u001b[0m     bros,\n\u001b[1;32m     42\u001b[0m     byt5,\n\u001b[1;32m     43\u001b[0m     camembert,\n\u001b[1;32m     44\u001b[0m     canine,\n\u001b[1;32m     45\u001b[0m     chameleon,\n\u001b[1;32m     46\u001b[0m     chinese_clip,\n\u001b[1;32m     47\u001b[0m     clap,\n\u001b[1;32m     48\u001b[0m     clip,\n\u001b[1;32m     49\u001b[0m     clipseg,\n\u001b[1;32m     50\u001b[0m     clvp,\n\u001b[1;32m     51\u001b[0m     code_llama,\n\u001b[1;32m     52\u001b[0m     codegen,\n\u001b[1;32m     53\u001b[0m     cohere,\n\u001b[1;32m     54\u001b[0m     conditional_detr,\n\u001b[1;32m     55\u001b[0m     convbert,\n\u001b[1;32m     56\u001b[0m     convnext,\n\u001b[1;32m     57\u001b[0m     convnextv2,\n\u001b[1;32m     58\u001b[0m     cpm,\n\u001b[1;32m     59\u001b[0m     cpmant,\n\u001b[1;32m     60\u001b[0m     ctrl,\n\u001b[1;32m     61\u001b[0m     cvt,\n\u001b[1;32m     62\u001b[0m     data2vec,\n\u001b[1;32m     63\u001b[0m     dbrx,\n\u001b[1;32m     64\u001b[0m     deberta,\n\u001b[1;32m     65\u001b[0m     deberta_v2,\n\u001b[1;32m     66\u001b[0m     decision_transformer,\n\u001b[1;32m     67\u001b[0m     deformable_detr,\n\u001b[1;32m     68\u001b[0m     deit,\n\u001b[1;32m     69\u001b[0m     deprecated,\n\u001b[1;32m     70\u001b[0m     depth_anything,\n\u001b[1;32m     71\u001b[0m     detr,\n\u001b[1;32m     72\u001b[0m     dialogpt,\n\u001b[1;32m     73\u001b[0m     dinat,\n\u001b[1;32m     74\u001b[0m     dinov2,\n\u001b[1;32m     75\u001b[0m     distilbert,\n\u001b[1;32m     76\u001b[0m     dit,\n\u001b[1;32m     77\u001b[0m     donut,\n\u001b[1;32m     78\u001b[0m     dpr,\n\u001b[1;32m     79\u001b[0m     dpt,\n\u001b[1;32m     80\u001b[0m     efficientnet,\n\u001b[1;32m     81\u001b[0m     electra,\n\u001b[1;32m     82\u001b[0m     encodec,\n\u001b[1;32m     83\u001b[0m     encoder_decoder,\n\u001b[1;32m     84\u001b[0m     ernie,\n\u001b[1;32m     85\u001b[0m     esm,\n\u001b[1;32m     86\u001b[0m     falcon,\n\u001b[1;32m     87\u001b[0m     fastspeech2_conformer,\n\u001b[1;32m     88\u001b[0m     flaubert,\n\u001b[1;32m     89\u001b[0m     flava,\n\u001b[1;32m     90\u001b[0m     fnet,\n\u001b[1;32m     91\u001b[0m     focalnet,\n\u001b[1;32m     92\u001b[0m     fsmt,\n\u001b[1;32m     93\u001b[0m     funnel,\n\u001b[1;32m     94\u001b[0m     fuyu,\n\u001b[1;32m     95\u001b[0m     gemma,\n\u001b[1;32m     96\u001b[0m     gemma2,\n\u001b[1;32m     97\u001b[0m     git,\n\u001b[1;32m     98\u001b[0m     glpn,\n\u001b[1;32m     99\u001b[0m     gpt2,\n\u001b[1;32m    100\u001b[0m     gpt_bigcode,\n\u001b[1;32m    101\u001b[0m     gpt_neo,\n\u001b[1;32m    102\u001b[0m     gpt_neox,\n\u001b[1;32m    103\u001b[0m     gpt_neox_japanese,\n\u001b[1;32m    104\u001b[0m     gpt_sw3,\n\u001b[1;32m    105\u001b[0m     gptj,\n\u001b[1;32m    106\u001b[0m     grounding_dino,\n\u001b[1;32m    107\u001b[0m     groupvit,\n\u001b[1;32m    108\u001b[0m     herbert,\n\u001b[1;32m    109\u001b[0m     hiera,\n\u001b[1;32m    110\u001b[0m     hubert,\n\u001b[1;32m    111\u001b[0m     ibert,\n\u001b[1;32m    112\u001b[0m     idefics,\n\u001b[1;32m    113\u001b[0m     idefics2,\n\u001b[1;32m    114\u001b[0m     imagegpt,\n\u001b[1;32m    115\u001b[0m     informer,\n\u001b[1;32m    116\u001b[0m     instructblip,\n\u001b[1;32m    117\u001b[0m     instructblipvideo,\n\u001b[1;32m    118\u001b[0m     jamba,\n\u001b[1;32m    119\u001b[0m     jetmoe,\n\u001b[1;32m    120\u001b[0m     kosmos2,\n\u001b[1;32m    121\u001b[0m     layoutlm,\n\u001b[1;32m    122\u001b[0m     layoutlmv2,\n\u001b[1;32m    123\u001b[0m     layoutlmv3,\n\u001b[1;32m    124\u001b[0m     layoutxlm,\n\u001b[1;32m    125\u001b[0m     led,\n\u001b[1;32m    126\u001b[0m     levit,\n\u001b[1;32m    127\u001b[0m     lilt,\n\u001b[1;32m    128\u001b[0m     llama,\n\u001b[1;32m    129\u001b[0m     llava,\n\u001b[1;32m    130\u001b[0m     llava_next,\n\u001b[1;32m    131\u001b[0m     llava_next_video,\n\u001b[1;32m    132\u001b[0m     longformer,\n\u001b[1;32m    133\u001b[0m     longt5,\n\u001b[1;32m    134\u001b[0m     luke,\n\u001b[1;32m    135\u001b[0m     lxmert,\n\u001b[1;32m    136\u001b[0m     m2m_100,\n\u001b[1;32m    137\u001b[0m     mamba,\n\u001b[1;32m    138\u001b[0m     mamba2,\n\u001b[1;32m    139\u001b[0m     marian,\n\u001b[1;32m    140\u001b[0m     markuplm,\n\u001b[1;32m    141\u001b[0m     mask2former,\n\u001b[1;32m    142\u001b[0m     maskformer,\n\u001b[1;32m    143\u001b[0m     mbart,\n\u001b[1;32m    144\u001b[0m     mbart50,\n\u001b[1;32m    145\u001b[0m     megatron_bert,\n\u001b[1;32m    146\u001b[0m     megatron_gpt2,\n\u001b[1;32m    147\u001b[0m     mgp_str,\n\u001b[1;32m    148\u001b[0m     mistral,\n\u001b[1;32m    149\u001b[0m     mixtral,\n\u001b[1;32m    150\u001b[0m     mluke,\n\u001b[1;32m    151\u001b[0m     mobilebert,\n\u001b[1;32m    152\u001b[0m     mobilenet_v1,\n\u001b[1;32m    153\u001b[0m     mobilenet_v2,\n\u001b[1;32m    154\u001b[0m     mobilevit,\n\u001b[1;32m    155\u001b[0m     mobilevitv2,\n\u001b[1;32m    156\u001b[0m     mpnet,\n\u001b[1;32m    157\u001b[0m     mpt,\n\u001b[1;32m    158\u001b[0m     mra,\n\u001b[1;32m    159\u001b[0m     mt5,\n\u001b[1;32m    160\u001b[0m     musicgen,\n\u001b[1;32m    161\u001b[0m     musicgen_melody,\n\u001b[1;32m    162\u001b[0m     mvp,\n\u001b[1;32m    163\u001b[0m     nemotron,\n\u001b[1;32m    164\u001b[0m     nllb,\n\u001b[1;32m    165\u001b[0m     nllb_moe,\n\u001b[1;32m    166\u001b[0m     nougat,\n\u001b[1;32m    167\u001b[0m     nystromformer,\n\u001b[1;32m    168\u001b[0m     olmo,\n\u001b[1;32m    169\u001b[0m     oneformer,\n\u001b[1;32m    170\u001b[0m     openai,\n\u001b[1;32m    171\u001b[0m     opt,\n\u001b[1;32m    172\u001b[0m     owlv2,\n\u001b[1;32m    173\u001b[0m     owlvit,\n\u001b[1;32m    174\u001b[0m     paligemma,\n\u001b[1;32m    175\u001b[0m     patchtsmixer,\n\u001b[1;32m    176\u001b[0m     patchtst,\n\u001b[1;32m    177\u001b[0m     pegasus,\n\u001b[1;32m    178\u001b[0m     pegasus_x,\n\u001b[1;32m    179\u001b[0m     perceiver,\n\u001b[1;32m    180\u001b[0m     persimmon,\n\u001b[1;32m    181\u001b[0m     phi,\n\u001b[1;32m    182\u001b[0m     phi3,\n\u001b[1;32m    183\u001b[0m     phobert,\n\u001b[1;32m    184\u001b[0m     pix2struct,\n\u001b[1;32m    185\u001b[0m     plbart,\n\u001b[1;32m    186\u001b[0m     poolformer,\n\u001b[1;32m    187\u001b[0m     pop2piano,\n\u001b[1;32m    188\u001b[0m     prophetnet,\n\u001b[1;32m    189\u001b[0m     pvt,\n\u001b[1;32m    190\u001b[0m     pvt_v2,\n\u001b[1;32m    191\u001b[0m     qwen2,\n\u001b[1;32m    192\u001b[0m     qwen2_moe,\n\u001b[1;32m    193\u001b[0m     rag,\n\u001b[1;32m    194\u001b[0m     recurrent_gemma,\n\u001b[1;32m    195\u001b[0m     reformer,\n\u001b[1;32m    196\u001b[0m     regnet,\n\u001b[1;32m    197\u001b[0m     rembert,\n\u001b[1;32m    198\u001b[0m     resnet,\n\u001b[1;32m    199\u001b[0m     roberta,\n\u001b[1;32m    200\u001b[0m     roberta_prelayernorm,\n\u001b[1;32m    201\u001b[0m     roc_bert,\n\u001b[1;32m    202\u001b[0m     roformer,\n\u001b[1;32m    203\u001b[0m     rt_detr,\n\u001b[1;32m    204\u001b[0m     rwkv,\n\u001b[1;32m    205\u001b[0m     sam,\n\u001b[1;32m    206\u001b[0m     seamless_m4t,\n\u001b[1;32m    207\u001b[0m     seamless_m4t_v2,\n\u001b[1;32m    208\u001b[0m     segformer,\n\u001b[1;32m    209\u001b[0m     seggpt,\n\u001b[1;32m    210\u001b[0m     sew,\n\u001b[1;32m    211\u001b[0m     sew_d,\n\u001b[1;32m    212\u001b[0m     siglip,\n\u001b[1;32m    213\u001b[0m     speech_encoder_decoder,\n\u001b[1;32m    214\u001b[0m     speech_to_text,\n\u001b[1;32m    215\u001b[0m     speecht5,\n\u001b[1;32m    216\u001b[0m     splinter,\n\u001b[1;32m    217\u001b[0m     squeezebert,\n\u001b[1;32m    218\u001b[0m     stablelm,\n\u001b[1;32m    219\u001b[0m     starcoder2,\n\u001b[1;32m    220\u001b[0m     superpoint,\n\u001b[1;32m    221\u001b[0m     swiftformer,\n\u001b[1;32m    222\u001b[0m     swin,\n\u001b[1;32m    223\u001b[0m     swin2sr,\n\u001b[1;32m    224\u001b[0m     swinv2,\n\u001b[1;32m    225\u001b[0m     switch_transformers,\n\u001b[1;32m    226\u001b[0m     t5,\n\u001b[1;32m    227\u001b[0m     table_transformer,\n\u001b[1;32m    228\u001b[0m     tapas,\n\u001b[1;32m    229\u001b[0m     time_series_transformer,\n\u001b[1;32m    230\u001b[0m     timesformer,\n\u001b[1;32m    231\u001b[0m     timm_backbone,\n\u001b[1;32m    232\u001b[0m     trocr,\n\u001b[1;32m    233\u001b[0m     tvp,\n\u001b[1;32m    234\u001b[0m     udop,\n\u001b[1;32m    235\u001b[0m     umt5,\n\u001b[1;32m    236\u001b[0m     unispeech,\n\u001b[1;32m    237\u001b[0m     unispeech_sat,\n\u001b[1;32m    238\u001b[0m     univnet,\n\u001b[1;32m    239\u001b[0m     upernet,\n\u001b[1;32m    240\u001b[0m     video_llava,\n\u001b[1;32m    241\u001b[0m     videomae,\n\u001b[1;32m    242\u001b[0m     vilt,\n\u001b[1;32m    243\u001b[0m     vipllava,\n\u001b[1;32m    244\u001b[0m     vision_encoder_decoder,\n\u001b[1;32m    245\u001b[0m     vision_text_dual_encoder,\n\u001b[1;32m    246\u001b[0m     visual_bert,\n\u001b[1;32m    247\u001b[0m     vit,\n\u001b[1;32m    248\u001b[0m     vit_mae,\n\u001b[1;32m    249\u001b[0m     vit_msn,\n\u001b[1;32m    250\u001b[0m     vitdet,\n\u001b[1;32m    251\u001b[0m     vitmatte,\n\u001b[1;32m    252\u001b[0m     vits,\n\u001b[1;32m    253\u001b[0m     vivit,\n\u001b[1;32m    254\u001b[0m     wav2vec2,\n\u001b[1;32m    255\u001b[0m     wav2vec2_bert,\n\u001b[1;32m    256\u001b[0m     wav2vec2_conformer,\n\u001b[1;32m    257\u001b[0m     wav2vec2_phoneme,\n\u001b[1;32m    258\u001b[0m     wav2vec2_with_lm,\n\u001b[1;32m    259\u001b[0m     wavlm,\n\u001b[1;32m    260\u001b[0m     whisper,\n\u001b[1;32m    261\u001b[0m     x_clip,\n\u001b[1;32m    262\u001b[0m     xglm,\n\u001b[1;32m    263\u001b[0m     xlm,\n\u001b[1;32m    264\u001b[0m     xlm_roberta,\n\u001b[1;32m    265\u001b[0m     xlm_roberta_xl,\n\u001b[1;32m    266\u001b[0m     xlnet,\n\u001b[1;32m    267\u001b[0m     xmod,\n\u001b[1;32m    268\u001b[0m     yolos,\n\u001b[1;32m    269\u001b[0m     yoso,\n\u001b[1;32m    270\u001b[0m     zoedepth,\n\u001b[1;32m    271\u001b[0m )\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "base_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "from pyfunctions.cdt_basic import *\n",
    "from pyfunctions.cdt_source_to_target import *\n",
    "from pyfunctions.ioi_dataset import IOIDataset\n",
    "from pyfunctions.wrappers import Node, AblationSet\n",
    "from pyfunctions.faithfulness_ablations import logits_to_ave_logit_diff_2, add_mean_ablation_hook\n",
    "\n",
    "\n",
    "Result = collections.namedtuple('Result', ('ablation_set', 'score'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7651660-7f59-4b59-a574-afecc52dc306",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520f760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "# Model code adapted from Callum McDougall's notebook for ARENA on reproducing the IOI paper using TransformerLens.\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\",\n",
    "                                          center_unembed=True,\n",
    "                                          center_writing_weights=True,\n",
    "                                          fold_ln=False,\n",
    "                                          refactor_factored_attn_matrices=True)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffd4d3-5e8f-4587-bb2a-f06b61918c09",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Generate mean activations / Example usage of the IOI dataset\n",
    "\n",
    "This is not as simple as it sounds; for the IOI paper, for each individual input following a template, they ablate using the mean activations of the \"ABC\" dataset, generated over sentences following the same template.\n",
    "\n",
    "For those who are familiar with usage of the IOI dataset code, our code is not designed to take advantage of the IOI dataset's sequence position labels (it fundamentally can't be because our method is automated and therefore can't incorporate knowledge of the sequence position labels, i.e, we can find that unlabeled positions are relevant), so circuit analysis needs to be done on a per-template basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab88048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a dataset all consisting of one template, randomly chosen.\n",
    "# nb_templates = 2 due to some logic internal to IOIDataset:\n",
    "# essentially, the nouns can be an ABBA or ABAB order and that counts as separate templates.\n",
    "ioi_dataset = IOIDataset(prompt_type=\"mixed\", N=50, tokenizer=model.tokenizer, prepend_bos=False, nb_templates=2)\n",
    "\n",
    "# This is the P_ABC that is mentioned in the IOI paper, which we use for mean ablation.\n",
    "# Importantly, passing in prompt_type=\"ABC\" or similar is NOT the same thing as this.\n",
    "abc_dataset = (\n",
    "    ioi_dataset.gen_flipped_prompts((\"IO\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S1\", \"RAND\"))\n",
    ")\n",
    "\n",
    "logits, cache = model.run_with_cache(abc_dataset.toks) # run on entire dataset along batch dimension\n",
    "\n",
    "# A technical detail: We patch at what TLens calls the \"z\" activation in the attention, which if you think about it is the only natural way to patch attention outputs on a per-head basis with the standard attention implementation that doesn't have a separate dimension for attention heads.\n",
    "attention_outputs = [cache['blocks.' + str(i) + '.attn.hook_z'] for i in range(12)]\n",
    "attention_outputs = torch.stack(attention_outputs, dim=1) # now batch, layer, seq, n_heads, dim_attn\n",
    "mean_acts = torch.mean(attention_outputs, dim=0)\n",
    "\n",
    "# different implementations of attention have a separate dimension for the attention heads, and we need to make sure the shapes are as expected.\n",
    "old_shape = mean_acts.shape\n",
    "last_dim = old_shape[-2] * old_shape[-1]\n",
    "new_shape = old_shape[:-2] + (last_dim,)\n",
    "mean_acts = mean_acts.view(new_shape)\n",
    "mean_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c32de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = ioi_dataset.sentences[0]\n",
    "encoding = model.tokenizer.encode_plus(text, \n",
    "                                 add_special_tokens=True, \n",
    "                                 max_length=512,\n",
    "                                 truncation=True, \n",
    "                                 padding = \"longest\", \n",
    "                                 return_attention_mask=True, \n",
    "                                 return_tensors=\"pt\").to(device)\n",
    "encoding_idxs, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "input_shape = encoding_idxs.size()\n",
    "extended_attention_mask = get_extended_attention_mask(attention_mask, \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea7c8f-a673-4b44-b9a9-d8e7615b3ec5",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Analysis\n",
    "\n",
    "These cells define the two basic operations of our method: decomposing the contribution directly to the logits, and decomposing the contribution to given target nodes.\n",
    "If you want to perform a specific analysis that requires some degree of human intervention or heuristic pruning, these cells are the place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2e4613-362f-480e-95fb-4f353ac3a845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running input 0\n",
      "running input 1600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ranges = [\n",
    "        [layer for layer in range(12)],\n",
    "        [sequence_position for sequence_position in range(input_shape[1])],\n",
    "        [attention_head_idx for attention_head_idx in range(12)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "ablation_sets = [(n,) for n in source_nodes]\n",
    "\n",
    "target_nodes = []\n",
    "\n",
    "# cache activations for faster batch run\n",
    "out_decomp, _, _, pre_layer_activations = prop_GPT(encoding_idxs[0:1, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True)\n",
    "\n",
    "prop_fn = lambda ablation_list: prop_GPT(encoding_idxs[0:1, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "out_decomps, target_decomps = batch_run(prop_fn, ablation_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b6f2e3-76e4-4f71-8f57-a87e6cbea335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_logits_decomposition_scores(out_decomps):\n",
    "    logits = (out_decomps[0].rel + out_decomps[0].irrel) # 1, seq_len, 50257=d_vocab\n",
    "    io_logit = logits[0, -2, ioi_dataset.io_tokenIDs[0]]\n",
    "    s_logit = logits[0, -2, ioi_dataset.s_tokenIDs[0]]\n",
    "    full_score = np.abs(io_logit - s_logit)\n",
    "    assert(full_score > 0) # GPT2 succeeds at this 99%+ of the time but not always. If you are doing analysis over a batch it mostly won't make a difference.\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for decomp in out_decomps:\n",
    "        rel_io_logit = decomp.rel[0, -2, ioi_dataset.io_tokenIDs[0]]\n",
    "        rel_s_logit = decomp.rel[0, -2, ioi_dataset.s_tokenIDs[0]]\n",
    "        score = rel_io_logit - rel_s_logit\n",
    "        results.append(Result(decomp.ablation_set, score))\n",
    "    results.sort(key=operator.attrgetter('score'), reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca897ab1-f348-4a6e-a43a-96f0fc30c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compute_logits_decomposition_scores(out_decomps)\n",
    "\n",
    "for result in results[:10]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fb15008-7d61-4bfd-b0e4-7126bd08951a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers_per_iter = []\n",
    "results_per_iter = [results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32b2b05c-eee7-4a95-aef5-126b082ec550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node(layer_idx=1, sequence_idx=3, attn_head_idx=7), Node(layer_idx=1, sequence_idx=3, attn_head_idx=10)]\n",
      "Running inputs 0 to 64 (of 2304)\n",
      "Running inputs 64 to 128 (of 2304)\n",
      "Running inputs 128 to 192 (of 2304)\n",
      "Running inputs 192 to 256 (of 2304)\n",
      "Running inputs 256 to 320 (of 2304)\n",
      "Running inputs 320 to 384 (of 2304)\n",
      "Running inputs 384 to 448 (of 2304)\n",
      "Running inputs 448 to 512 (of 2304)\n",
      "Running inputs 512 to 576 (of 2304)\n",
      "Running inputs 576 to 640 (of 2304)\n",
      "Running inputs 640 to 704 (of 2304)\n",
      "Running inputs 704 to 768 (of 2304)\n",
      "Running inputs 768 to 832 (of 2304)\n",
      "Running inputs 832 to 896 (of 2304)\n",
      "Running inputs 896 to 960 (of 2304)\n",
      "Running inputs 960 to 1024 (of 2304)\n",
      "Running inputs 1024 to 1088 (of 2304)\n",
      "Running inputs 1088 to 1152 (of 2304)\n",
      "Running inputs 1152 to 1216 (of 2304)\n",
      "Running inputs 1216 to 1280 (of 2304)\n",
      "Running inputs 1280 to 1344 (of 2304)\n",
      "Running inputs 1344 to 1408 (of 2304)\n",
      "Running inputs 1408 to 1472 (of 2304)\n",
      "Running inputs 1472 to 1536 (of 2304)\n",
      "Running inputs 1536 to 1600 (of 2304)\n",
      "Running inputs 1600 to 1664 (of 2304)\n",
      "Running inputs 1664 to 1728 (of 2304)\n",
      "Running inputs 1728 to 1792 (of 2304)\n",
      "Running inputs 1792 to 1856 (of 2304)\n",
      "Running inputs 1856 to 1920 (of 2304)\n",
      "Running inputs 1920 to 1984 (of 2304)\n",
      "Running inputs 1984 to 2048 (of 2304)\n",
      "Running inputs 2048 to 2112 (of 2304)\n",
      "Running inputs 2112 to 2176 (of 2304)\n",
      "Running inputs 2176 to 2240 (of 2304)\n",
      "Running inputs 2240 to 2304 (of 2304)\n"
     ]
    }
   ],
   "source": [
    "# Now, find maximally relevant source nodes to target nodes\n",
    "\n",
    "outliers = results[:2] # say we take top 2 heads in each iter\n",
    "outliers_per_iter.append(outliers)\n",
    "target_nodes = [r.ablation_set[0] for r in outliers] # here we assume that we only ever tried to ablate one node at once, but our code support multiple node ablation at once too\n",
    "print(target_nodes)\n",
    "ranges = [\n",
    "        [layer for layer in range(12)],\n",
    "        [sequence_position for sequence_position in range(16)],\n",
    "        # [ioi_dataset.word_idx['IO'][0]],\n",
    "        [attention_head_idx for attention_head_idx in range(12)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "ablation_sets = [(n,) for n in source_nodes]\n",
    "\n",
    "_, _, _, pre_layer_activations = prop_GPT(encoding_idxs[0:1, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True)\n",
    "\n",
    "prop_fn = lambda ablation_list: prop_GPT(encoding_idxs[0:1, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "out_decomps, target_decomps = batch_run(prop_fn, ablation_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ebab7b5-5f80-40b6-98f1-bccf4a16274a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_target_decomposition_scores(target_decomps, method=\"l1\", mean_acts=None, attn_cache=None):\n",
    "    results = []\n",
    "    relevances = np.zeros((12, 16, 12))\n",
    "    for layer in range(12):\n",
    "        for sequence_position in range(16):\n",
    "            for attention_head_idx in range(12):\n",
    "                idx = layer * 16 * 12 + sequence_position * 12 + attention_head_idx\n",
    "                target_decomp = target_decomps[idx]\n",
    "                if target_decomp.ablation_set[0] in target_nodes:\n",
    "                    continue\n",
    "                score = 0\n",
    "                for i in range(len(target_decomp.target_nodes)):\n",
    "                    if method == 'l1':\n",
    "                        rels_magnitude = torch.mean(abs(target_decomp.rels[i])) # np.mean if you are on cpu\n",
    "                        irrels_magnitude = torch.mean(abs(target_decomp.irrels[i])) # np.mean if you are on cpu\n",
    "                        target_node_score = rels_magnitude / (rels_magnitude + irrels_magnitude)\n",
    "                        score += target_node_score\n",
    "                    if method == 'dot':\n",
    "                        target_node = target_decomp.target_nodes[i]\n",
    "                        # this method is only implemented for a single datapoint\n",
    "                        if mean_acts is None or attn_cache is None:\n",
    "                            print(\"Invalid target decomposition score calculation\") # and then this is going to crash anyway\n",
    "                        target_mean_act = mean_acts[target_node.layer_idx, target_node.sequence_idx, target_node.attn_head_idx]\n",
    "                        target_rel = attn_cache['blocks.' + str(target_node.layer_idx) + '.attn.hook_z'][0][target_node.sequence_idx][target_node.attn_head_idx] - target_mean_act \n",
    "                        rel = target_decomp.rels[i][0]\n",
    "                        #print(target_rel.shape, rel.shape)\n",
    "                        score += torch.dot(rel, target_rel)\n",
    "                relevances[layer, sequence_position, attention_head_idx] = score\n",
    "\n",
    "\n",
    "    sums_per_layer = np.sum(relevances, axis=(1, 2))\n",
    "    sums_per_layer[sums_per_layer == 0] = -1e-8\n",
    "    normalized_relevances = relevances / np.expand_dims(sums_per_layer, (1, 2))\n",
    "\n",
    "    num_layers = 12\n",
    "    seq_len = 16\n",
    "    num_attention_heads = 12\n",
    "    for layer_idx in range(num_layers):\n",
    "        for seq_pos in range(seq_len):\n",
    "            for head_idx in range(num_attention_heads):\n",
    "                target_decomp = target_decomps[layer_idx * seq_len * num_attention_heads + seq_pos * num_attention_heads + head_idx]\n",
    "                results.append(Result(target_decomp.ablation_set, normalized_relevances[layer_idx, seq_pos, head_idx]))\n",
    "\n",
    "    results.sort(key=operator.attrgetter('score'), reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ecd696fa-86b4-4ba3-862f-9be581d67a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Result(ablation_set=(Node(layer_idx=8, sequence_idx=14, attn_head_idx=6),), score=0.03813974573802492),\n",
       "  Result(ablation_set=(Node(layer_idx=8, sequence_idx=11, attn_head_idx=6),), score=0.034177570085989574)],\n",
       " [Result(ablation_set=(Node(layer_idx=8, sequence_idx=14, attn_head_idx=6),), score=0.03813974573802492),\n",
       "  Result(ablation_set=(Node(layer_idx=8, sequence_idx=11, attn_head_idx=6),), score=0.034177570085989574)],\n",
       " [Result(ablation_set=(Node(layer_idx=9, sequence_idx=14, attn_head_idx=9),), score=0.87486565),\n",
       "  Result(ablation_set=(Node(layer_idx=9, sequence_idx=14, attn_head_idx=6),), score=0.37573943)],\n",
       " [Result(ablation_set=(Node(layer_idx=8, sequence_idx=14, attn_head_idx=6),), score=0.03813974573802492),\n",
       "  Result(ablation_set=(Node(layer_idx=8, sequence_idx=11, attn_head_idx=6),), score=0.034177570085989574)],\n",
       " [Result(ablation_set=(Node(layer_idx=5, sequence_idx=10, attn_head_idx=5),), score=0.02986675545961833),\n",
       "  Result(ablation_set=(Node(layer_idx=7, sequence_idx=11, attn_head_idx=9),), score=0.02952390684446772)],\n",
       " [Result(ablation_set=(Node(layer_idx=6, sequence_idx=10, attn_head_idx=9),), score=0.04139306313448273),\n",
       "  Result(ablation_set=(Node(layer_idx=6, sequence_idx=11, attn_head_idx=0),), score=0.03349508347071345)],\n",
       " [Result(ablation_set=(Node(layer_idx=5, sequence_idx=10, attn_head_idx=5),), score=0.11361695632022674),\n",
       "  Result(ablation_set=(Node(layer_idx=5, sequence_idx=10, attn_head_idx=9),), score=0.05332546980497713)],\n",
       " [Result(ablation_set=(Node(layer_idx=3, sequence_idx=10, attn_head_idx=0),), score=0.038520373769848806),\n",
       "  Result(ablation_set=(Node(layer_idx=4, sequence_idx=5, attn_head_idx=11),), score=0.02788752213108418)],\n",
       " [Result(ablation_set=(Node(layer_idx=3, sequence_idx=5, attn_head_idx=7),), score=0.05608457333323692),\n",
       "  Result(ablation_set=(Node(layer_idx=3, sequence_idx=3, attn_head_idx=6),), score=0.05386950067347974)],\n",
       " [Result(ablation_set=(Node(layer_idx=2, sequence_idx=3, attn_head_idx=2),), score=0.10140302955641779),\n",
       "  Result(ablation_set=(Node(layer_idx=2, sequence_idx=3, attn_head_idx=9),), score=0.05820936329902795)],\n",
       " [Result(ablation_set=(Node(layer_idx=1, sequence_idx=3, attn_head_idx=7),), score=0.081322716382124),\n",
       "  Result(ablation_set=(Node(layer_idx=1, sequence_idx=3, attn_head_idx=10),), score=0.06703445480573866)],\n",
       " [Result(ablation_set=(Node(layer_idx=0, sequence_idx=2, attn_head_idx=1),), score=0.08746183454945368),\n",
       "  Result(ablation_set=(Node(layer_idx=0, sequence_idx=2, attn_head_idx=4),), score=0.07542276779756175)]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers_per_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "587a4627-5729-4810-aaad-5445007a9869",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=2, attn_head_idx=1),), score=0.08746183454945368)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=2, attn_head_idx=4),), score=0.07542276779756175)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=3, attn_head_idx=6),), score=0.07055409286121249)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=2, attn_head_idx=5),), score=0.06713339385727347)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=2, attn_head_idx=3),), score=0.05414199889043761)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=3, attn_head_idx=7),), score=0.049619719961781564)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=2, attn_head_idx=6),), score=0.04932286453511382)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=2, attn_head_idx=10),), score=0.046369913283579374)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=3, attn_head_idx=0),), score=0.042463772022659954)\n",
      "Result(ablation_set=(Node(layer_idx=0, sequence_idx=3, attn_head_idx=4),), score=0.03883541583656693)\n"
     ]
    }
   ],
   "source": [
    "for result in results[:10]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "15844abd-6da0-491c-b8f3-b58444a0f11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "for it in outliers_per_iter:\n",
    "    for result in it:\n",
    "        if result.ablation_set[0] not in all_nodes:\n",
    "            all_nodes.append(result.ablation_set[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e32e8-925d-4285-b6db-d54e6854a531",
   "metadata": {},
   "source": [
    "## Automatic search\n",
    "\n",
    "This is just a bunch of the above cells put into a neat cell that automatically finds some sort of circuit without any manual intervention.\n",
    "As explained above, the code is not designed to take advantage of the IOI dataset's sequence position labels, so circuit analysis needs to be done on a per-template basis; here a template is hardcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05babeb3-e7fb-414b-8ad4-0701b77cfd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence length: 16 \n",
      "running input 0\n",
      "running input 1600\n",
      "[Node(layer_idx=9, sequence_idx=14, attn_head_idx=9), Node(layer_idx=10, sequence_idx=14, attn_head_idx=10)]\n",
      "running input 0\n",
      "running input 1600\n",
      "[Node(layer_idx=9, sequence_idx=14, attn_head_idx=6), Node(layer_idx=9, sequence_idx=2, attn_head_idx=6)]\n",
      "running input 0\n",
      "running input 1600\n",
      "[Node(layer_idx=0, sequence_idx=1, attn_head_idx=1), Node(layer_idx=0, sequence_idx=1, attn_head_idx=4)]\n"
     ]
    }
   ],
   "source": [
    "from pyfunctions.ioi_dataset import ABC_TEMPLATES, BAC_TEMPLATES, BABA_TEMPLATES, BABA_LONG_TEMPLATES, BABA_LATE_IOS, BABA_EARLY_IOS, ABBA_TEMPLATES, ABBA_LATE_IOS, ABBA_EARLY_IOS\n",
    "\n",
    "model.reset_hooks(including_permanent=True)\n",
    "\n",
    "NUM_SAMPLES = 1\n",
    "NUM_OUTLIERS_TO_KEEP_PER_ITER = 2\n",
    "template = ABBA_EARLY_IOS[0]\n",
    "ioi_dataset = IOIDataset(N=50, tokenizer=model.tokenizer, prepend_bos=False, prompt_type=[template])\n",
    "\n",
    "# This is the P_ABC that is mentioned in the IOI paper, which we use for mean ablation.\n",
    "# Importantly, passing in prompt_type=\"ABC\" or similar is NOT the same thing as this.\n",
    "abc_dataset = (\n",
    "    ioi_dataset.gen_flipped_prompts((\"IO\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S1\", \"RAND\"))\n",
    ")\n",
    "ioi_logits, ioi_cache = model.run_with_cache(ioi_dataset.toks) # run on entire dataset along batch dimension\n",
    "logits, cache = model.run_with_cache(abc_dataset.toks) # run on entire dataset along batch dimension\n",
    "\n",
    "attention_outputs = [cache['blocks.' + str(i) + '.attn.hook_z'] for i in range(12)]\n",
    "attention_outputs = torch.stack(attention_outputs, dim=1) # now batch, layer, seq, n_heads, dim_attn\n",
    "mean_acts = torch.mean(attention_outputs, dim=0)\n",
    "old_shape = mean_acts.shape\n",
    "last_dim = old_shape[-2] * old_shape[-1]\n",
    "new_shape = old_shape[:-2] + (last_dim,)\n",
    "mean_acts = mean_acts.view(new_shape)\n",
    "\n",
    "text = ioi_dataset.sentences[0]\n",
    "encoding = model.tokenizer.encode_plus(text, \n",
    "                                 add_special_tokens=True, \n",
    "                                 max_length=512,\n",
    "                                 truncation=True, \n",
    "                                 padding = \"longest\", \n",
    "                                 return_attention_mask=True, \n",
    "                                 return_tensors=\"pt\").to(device)\n",
    "input_shape = encoding.input_ids.size()\n",
    "extended_attention_mask = get_extended_attention_mask(encoding.attention_mask, \n",
    "                                                        input_shape, \n",
    "                                                        model,\n",
    "                                                        device)\n",
    "seq_len = ioi_dataset.toks.shape[1]\n",
    "print('sequence length: %d ' % seq_len)\n",
    "# Calculate relevance to logits\n",
    "ranges = [\n",
    "        [layer for layer in range(12)],\n",
    "        [sequence_position for sequence_position in range(seq_len)],\n",
    "        [attention_head_idx for attention_head_idx in range(12)]\n",
    "    ]\n",
    "\n",
    "source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "ablation_sets = [(n,) for n in source_nodes]\n",
    "target_nodes = []\n",
    "\n",
    "# cache activations for faster batch run\n",
    "out_decomp, _, _, pre_layer_activations = prop_GPT(ioi_dataset.toks[0:NUM_SAMPLES, :], extended_attention_mask, model, [ablation_sets[0]], target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True)\n",
    "prop_fn = lambda ablation_list: prop_GPT(ioi_dataset.toks[0:NUM_SAMPLES, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "out_decomps, _ = batch_run(prop_fn, ablation_sets, num_at_time=(64 // NUM_SAMPLES))\n",
    "results = compute_logits_decomposition_scores(out_decomps)\n",
    "\n",
    "\n",
    "# This loop implements a simple heuristic of keeping a hardcoded top N outliers from each iteration.\n",
    "# It terminates when all the nodes are in the first layer, so it has the shortcoming of continually trying to find nodes even when they are not necessarily important.\n",
    "# Various heuristic techniques, such as filtering nodes by how their relevance scores compare to others in the same layer, or same iteration, can be applied.\n",
    "# It is also possible to implement early stopping or other heuristic techniques based on the circuit's performance.\n",
    "\n",
    "\n",
    "outliers_per_iter = []\n",
    "while True:\n",
    "    outliers = results[:NUM_OUTLIERS_TO_KEEP_PER_ITER]\n",
    "    outliers_per_iter.append(outliers)\n",
    "    target_nodes = [r.ablation_set[0] for r in outliers]\n",
    "    print(target_nodes)\n",
    "    should_break = True\n",
    "    for node in target_nodes:\n",
    "        if node.layer_idx != 0:\n",
    "            should_break = False\n",
    "    if should_break:\n",
    "        break\n",
    "\n",
    "    # In this loop, we implement search over all sequence positions.\n",
    "    # This result is less stable than the one augmented by some amount of manual analysis.\n",
    "    ranges = [\n",
    "            [layer for layer in range(12)],\n",
    "            [sequence_position for sequence_position in range(seq_len)],\n",
    "            [attention_head_idx for attention_head_idx in range(12)]\n",
    "        ]\n",
    "    source_nodes = [Node(*x) for x in itertools.product(*ranges)]\n",
    "    ablation_sets = [(n,) for n in source_nodes]\n",
    "    prop_fn = lambda ablation_list: prop_GPT(ioi_dataset.toks[0:NUM_SAMPLES, :], extended_attention_mask, model, ablation_list, target_nodes=target_nodes, device=device, mean_acts=mean_acts, set_irrel_to_mean=True, cached_pre_layer_acts=pre_layer_activations)\n",
    "    _, target_decomps = batch_run(prop_fn, ablation_sets, num_at_time=(64 // NUM_SAMPLES))\n",
    "    \n",
    "    results = calculate_target_decomposition_scores(target_decomps, method=\"dot\", mean_acts=mean_acts.view(old_shape), attn_cache=ioi_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15bb0c2f-bef4-4bc3-8f5a-f2177a17e538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node(layer_idx=9, sequence_idx=14, attn_head_idx=9)\n",
      "Node(layer_idx=10, sequence_idx=14, attn_head_idx=10)\n",
      "Node(layer_idx=9, sequence_idx=14, attn_head_idx=6)\n",
      "Node(layer_idx=9, sequence_idx=2, attn_head_idx=6)\n",
      "Node(layer_idx=0, sequence_idx=1, attn_head_idx=1)\n",
      "Node(layer_idx=0, sequence_idx=1, attn_head_idx=4)\n"
     ]
    }
   ],
   "source": [
    "all_nodes = []\n",
    "for it in outliers_per_iter:\n",
    "    for result in it:\n",
    "        if result.ablation_set[0] not in all_nodes:\n",
    "            all_nodes.append(result.ablation_set[0])\n",
    "for node in all_nodes:\n",
    "    print((node))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac83979",
   "metadata": {},
   "source": [
    "# Circuit evaluation\n",
    "\n",
    "Most of the actual evaluation code is implemented in the IOI repo; we just make calls to convenient functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3b526010-d4e4-41b3-9ff2-fe33d832e148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "circuit = [Node(layer_idx=8, sequence_idx=14, attn_head_idx=6),\n",
    "           Node(layer_idx=8, sequence_idx=11, attn_head_idx=6),\n",
    "           Node(layer_idx=9, sequence_idx=14, attn_head_idx=9),\n",
    "           Node(layer_idx=9, sequence_idx=14, attn_head_idx=6),\n",
    "           Node(layer_idx=5, sequence_idx=10, attn_head_idx=5),\n",
    "           Node(layer_idx=7, sequence_idx=11, attn_head_idx=9),\n",
    "           Node(layer_idx=6, sequence_idx=10, attn_head_idx=9),\n",
    "           Node(layer_idx=6, sequence_idx=11, attn_head_idx=0),\n",
    "           Node(layer_idx=5, sequence_idx=10, attn_head_idx=9),\n",
    "           Node(layer_idx=3, sequence_idx=10, attn_head_idx=0),\n",
    "           Node(layer_idx=4, sequence_idx=5, attn_head_idx=11),\n",
    "           Node(layer_idx=3, sequence_idx=5, attn_head_idx=7),\n",
    "           Node(layer_idx=3, sequence_idx=3, attn_head_idx=6),\n",
    "           Node(layer_idx=2, sequence_idx=3, attn_head_idx=2),\n",
    "           Node(layer_idx=2, sequence_idx=3, attn_head_idx=9),\n",
    "           Node(layer_idx=1, sequence_idx=3, attn_head_idx=7),\n",
    "           Node(layer_idx=1, sequence_idx=3, attn_head_idx=10),\n",
    "           Node(layer_idx=0, sequence_idx=2, attn_head_idx=1),\n",
    "           Node(layer_idx=0, sequence_idx=2, attn_head_idx=4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33374a25-c2a8-493e-8480-ecc2274e6ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_circuit = random.sample(source_nodes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9db57878-aaf0-4ef6-83ad-021902bf9e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.4686, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# This template definitely has to match the template used in the search above, otherwise, the sequence positions will not be validly interpretable.\n",
    "test_ioi_dataset = IOIDataset(prompt_type=[template], N=10, tokenizer=model.tokenizer, prepend_bos=False)\n",
    "test_abc_dataset = (\n",
    "    test_ioi_dataset.gen_flipped_prompts((\"IO\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S1\", \"RAND\"))\n",
    ")\n",
    "\n",
    "circuit = all_nodes\n",
    "\n",
    "model.reset_hooks(including_permanent=True)\n",
    "model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset, circuit=circuit) #, circuit=random_circuit)\n",
    "# model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset)\n",
    "logits, cache = model.run_with_cache(test_ioi_dataset.toks) # run on entire dataset along batch dimension\n",
    "ave_logit_diff = logits_to_ave_logit_diff_2(logits, test_ioi_dataset)\n",
    "print(ave_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7de9d9f5-2955-46b7-92bc-9f26308ce450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note: for the following circuit:\n",
    "'''\n",
    "Node(layer_idx=9, sequence_idx=14, attn_head_idx=9)\n",
    "Node(layer_idx=9, sequence_idx=14, attn_head_idx=6)\n",
    "Node(layer_idx=10, sequence_idx=14, attn_head_idx=0)\n",
    "Node(layer_idx=8, sequence_idx=1, attn_head_idx=11)\n",
    "Node(layer_idx=8, sequence_idx=1, attn_head_idx=10)\n",
    "Node(layer_idx=8, sequence_idx=1, attn_head_idx=2)\n",
    "Node(layer_idx=7, sequence_idx=1, attn_head_idx=1)\n",
    "Node(layer_idx=7, sequence_idx=1, attn_head_idx=4)\n",
    "Node(layer_idx=6, sequence_idx=1, attn_head_idx=4)\n",
    "Node(layer_idx=6, sequence_idx=1, attn_head_idx=0)\n",
    "Node(layer_idx=5, sequence_idx=1, attn_head_idx=10)\n",
    "Node(layer_idx=5, sequence_idx=1, attn_head_idx=2)\n",
    "Node(layer_idx=5, sequence_idx=1, attn_head_idx=3)\n",
    "Node(layer_idx=5, sequence_idx=1, attn_head_idx=6)\n",
    "Node(layer_idx=5, sequence_idx=1, attn_head_idx=9)\n",
    "Node(layer_idx=4, sequence_idx=1, attn_head_idx=3)\n",
    "Node(layer_idx=4, sequence_idx=1, attn_head_idx=10)\n",
    "Node(layer_idx=4, sequence_idx=1, attn_head_idx=9)\n",
    "Node(layer_idx=1, sequence_idx=1, attn_head_idx=3)\n",
    "Node(layer_idx=1, sequence_idx=1, attn_head_idx=10)\n",
    "Node(layer_idx=1, sequence_idx=1, attn_head_idx=4)\n",
    "Node(layer_idx=0, sequence_idx=1, attn_head_idx=3)\n",
    "Node(layer_idx=0, sequence_idx=1, attn_head_idx=4)\n",
    "Node(layer_idx=0, sequence_idx=1, attn_head_idx=5)\n",
    "'''\n",
    "# removing just one node, (8, 1, 11), raises the score from -2.1718 to -0.4423.\n",
    "# this node is not identified by the IOI paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04fa93",
   "metadata": {},
   "source": [
    "# Pruning heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune nodes by greedy search to form a better circuit\n",
    "\n",
    "NAME_MOVER_HEADS = [Node(9, 14, 9), Node(10, 14, 0), Node(9, 14, 6)]\n",
    "old_circuit = circuit.copy()\n",
    "best_score = -1.4686 # \n",
    "while True:\n",
    "    node_to_remove = None\n",
    "    for idx, node in enumerate(circuit):\n",
    "        if node in NAME_MOVER_HEADS:\n",
    "            continue\n",
    "        new_circuit = circuit.copy()\n",
    "        new_circuit.remove(node)\n",
    "        # print(new_circuit)\n",
    "        model.reset_hooks(including_permanent=True)\n",
    "        model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset, circuit=new_circuit)\n",
    "        logits, cache = model.run_with_cache(test_ioi_dataset.toks) # run on entire dataset along batch dimension\n",
    "        ave_logit_diff = logits_to_ave_logit_diff_2(logits, test_ioi_dataset).cpu().numpy().item()\n",
    "        if ave_logit_diff > best_score:\n",
    "            best_score = ave_logit_diff\n",
    "            node_to_remove = node\n",
    "            print('tentatively improved score to %f ' % best_score, ' by removing node ', node_to_remove)\n",
    "    if node_to_remove is None: \n",
    "        # then we can't improve any further so the algorithm terminates\n",
    "        break\n",
    "    print(\"removing \", node_to_remove, \" to achieve score of %f\" % best_score)\n",
    "    circuit.remove(node_to_remove)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e9717bb-1f4e-4823-bcfc-ea685daf8af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5994, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.reset_hooks(including_permanent=True)\n",
    "# model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset, circuit=circuit)\n",
    "model = add_mean_ablation_hook(model, means_dataset=test_abc_dataset, circuit=nodes)\n",
    "logits, cache = model.run_with_cache(test_ioi_dataset.toks) # run on entire dataset along batch dimension\n",
    "ave_logit_diff = logits_to_ave_logit_diff_2(logits, test_ioi_dataset)\n",
    "print(ave_logit_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (ipykernel)",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
